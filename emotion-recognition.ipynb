{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":20284,"status":"ok","timestamp":1688586444114,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"26669885","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c63a456-f956-4dc9-dea1-9089cbce7b13"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers pydub"],"id":"26669885"},{"cell_type":"markdown","metadata":{"id":"fb907ddf"},"source":["## Basic Import"],"id":"fb907ddf"},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13229,"status":"ok","timestamp":1688586457336,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"8ab6d70b"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","import torchvision.transforms as T\n","import librosa\n","import librosa.display\n","import IPython.display as ipd\n","import matplotlib.pyplot as plt\n","import random"],"id":"8ab6d70b"},{"cell_type":"markdown","metadata":{"id":"43cf6a1f"},"source":["# 0. Device Agnostic Code"],"id":"43cf6a1f"},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1688586457337,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"a30f23ca","outputId":"36e592ca-028e-4676-cd91-29175f40818a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jul  5 19:47:37 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"],"id":"a30f23ca"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1688586457338,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"d5727ba0","outputId":"9ce0010b-c886-43f8-8f7c-082fcf2dc111"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["# Set up device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"id":"d5727ba0"},{"cell_type":"markdown","metadata":{"id":"79577ce2"},"source":["# 1. Create DataFrame for the data"],"id":"79577ce2"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23055,"status":"ok","timestamp":1688586480385,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"q9_XiSLbu-NE","outputId":"cafa6773-0419-4f39-d80a-3759e8f029dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"q9_XiSLbu-NE"},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1688586480386,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"4d229fd8"},"outputs":[],"source":["TESS = '/content/drive/MyDrive/tess/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/'\n","RAV = '/content/drive/MyDrive/ravdess/'\n","SAVEE = '/content/drive/MyDrive/savee/ALL/'\n","CREMA = '/content/drive/MyDrive/cremad/AudioWAV/'"],"id":"4d229fd8"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3520,"status":"ok","timestamp":1688586483900,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"70f69c9a","outputId":"5d4f034c-54b4-4935-bff8-b9170fc30bb3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["male_neutral     120\n","male_angry        60\n","male_happy        60\n","male_disgust      60\n","male_fear         60\n","male_surprise     60\n","male_sad          60\n","Name: labels, dtype: int64"]},"metadata":{},"execution_count":7}],"source":["# Get the data location for SAVEE\n","dir_list = os.listdir(SAVEE)\n","\n","# parse the filename to get the emotions\n","emotion=[]\n","path = []\n","for i in dir_list:\n","    if i[-8:-6]=='_a':\n","        emotion.append('male_angry')\n","    elif i[-8:-6]=='_d':\n","        emotion.append('male_disgust')\n","    elif i[-8:-6]=='_f':\n","        emotion.append('male_fear')\n","    elif i[-8:-6]=='_h':\n","        emotion.append('male_happy')\n","    elif i[-8:-6]=='_n':\n","        emotion.append('male_neutral')\n","    elif i[-8:-6]=='sa':\n","        emotion.append('male_sad')\n","    elif i[-8:-6]=='su':\n","        emotion.append('male_surprise')\n","    else:\n","        emotion.append('male_error')\n","    path.append(SAVEE + i)\n","\n","# Now check out the label count distribution\n","SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n","SAVEE_df['source'] = 'SAVEE'\n","SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n","SAVEE_df.labels.value_counts()"],"id":"70f69c9a"},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1688586483901,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"73e4a6ae","outputId":"2cf174a5-6d55-4103-f3e0-4e31410e503c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["            labels source                                          path\n","0       male_angry  SAVEE   /content/drive/MyDrive/savee/ALL/DC_a01.wav\n","1       male_angry  SAVEE   /content/drive/MyDrive/savee/ALL/DC_a08.wav\n","2       male_angry  SAVEE   /content/drive/MyDrive/savee/ALL/DC_a09.wav\n","3       male_angry  SAVEE   /content/drive/MyDrive/savee/ALL/DC_a10.wav\n","4       male_angry  SAVEE   /content/drive/MyDrive/savee/ALL/DC_a02.wav\n","..             ...    ...                                           ...\n","475   male_neutral  SAVEE   /content/drive/MyDrive/savee/ALL/KL_n02.wav\n","476  male_surprise  SAVEE  /content/drive/MyDrive/savee/ALL/KL_su01.wav\n","477   male_neutral  SAVEE   /content/drive/MyDrive/savee/ALL/KL_n27.wav\n","478   male_neutral  SAVEE   /content/drive/MyDrive/savee/ALL/KL_n30.wav\n","479   male_neutral  SAVEE   /content/drive/MyDrive/savee/ALL/KL_n23.wav\n","\n","[480 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-c97ca4c7-99aa-42af-82c9-2452feab703d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","      <th>source</th>\n","      <th>path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a01.wav</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a08.wav</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a09.wav</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a10.wav</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a02.wav</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>475</th>\n","      <td>male_neutral</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/KL_n02.wav</td>\n","    </tr>\n","    <tr>\n","      <th>476</th>\n","      <td>male_surprise</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/KL_su01.wav</td>\n","    </tr>\n","    <tr>\n","      <th>477</th>\n","      <td>male_neutral</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/KL_n27.wav</td>\n","    </tr>\n","    <tr>\n","      <th>478</th>\n","      <td>male_neutral</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/KL_n30.wav</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>male_neutral</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/KL_n23.wav</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>480 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c97ca4c7-99aa-42af-82c9-2452feab703d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c97ca4c7-99aa-42af-82c9-2452feab703d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c97ca4c7-99aa-42af-82c9-2452feab703d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["SAVEE_df"],"id":"73e4a6ae"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1688586483901,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"16407229","outputId":"239a0971-3ce8-4963-8318-7e3c7bc0c98a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Actor_01',\n"," 'Actor_02',\n"," 'Actor_03',\n"," 'Actor_04',\n"," 'Actor_14',\n"," 'Actor_13',\n"," 'Actor_07',\n"," 'Actor_09',\n"," 'Actor_11',\n"," 'Actor_08',\n"," 'Actor_10',\n"," 'Actor_12',\n"," 'Actor_06',\n"," 'Actor_05',\n"," 'Actor_15',\n"," 'Actor_16',\n"," 'Actor_23',\n"," 'Actor_17',\n"," 'Actor_18',\n"," 'Actor_24',\n"," 'Actor_20',\n"," 'Actor_21',\n"," 'audio_speech_actors_01-24',\n"," 'Actor_22',\n"," 'Actor_19']"]},"metadata":{},"execution_count":9}],"source":["dir_list = os.listdir(RAV)\n","dir_list"],"id":"16407229"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4132,"status":"ok","timestamp":1688586488027,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"dce059d1","outputId":"4c69c56b-1367-4507-cb1e-653032004e3c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["male_neutral       144\n","female_neutral     144\n","male_sad            96\n","male_happy          96\n","male_angry          96\n","male_disgust        96\n","male_surprise       96\n","male_fear           96\n","female_happy        96\n","female_sad          96\n","female_angry        96\n","female_disgust      96\n","female_fear         96\n","female_surprise     96\n","Name: labels, dtype: int64"]},"metadata":{},"execution_count":10}],"source":["dir_list = os.listdir(RAV)\n","dir_list.sort()\n","\n","emotion = []\n","gender = []\n","path = []\n","for directory in dir_list:\n","    if directory == \"audio_speech_actors_01-24\":\n","        continue\n","    fname = os.listdir(RAV + directory)\n","    for f in fname:\n","        part = f.split('.')[0].split('-')\n","        emotion.append(int(part[2]))\n","        temp = int(part[6])\n","        if temp%2 == 0:\n","            temp = \"female\"\n","        else:\n","            temp = \"male\"\n","        gender.append(temp)\n","        path.append(RAV + directory + '/' + f)\n","\n","RAV_df = pd.DataFrame(emotion)\n","RAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n","RAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\n","RAV_df.columns = ['gender','emotion']\n","RAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\n","RAV_df['source'] = 'RAVDESS'\n","RAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n","RAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\n","RAV_df.labels.value_counts()"],"id":"dce059d1"},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1688586488028,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"103b71d5","outputId":"56fb1f20-8d74-4b1a-ef59-e8e0fd93bf40"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               labels   source  \\\n","0        male_neutral  RAVDESS   \n","1        male_neutral  RAVDESS   \n","2            male_sad  RAVDESS   \n","3          male_happy  RAVDESS   \n","4          male_angry  RAVDESS   \n","...               ...      ...   \n","1435     female_angry  RAVDESS   \n","1436  female_surprise  RAVDESS   \n","1437  female_surprise  RAVDESS   \n","1438  female_surprise  RAVDESS   \n","1439  female_surprise  RAVDESS   \n","\n","                                                   path  \n","0     /content/drive/MyDrive/ravdess/Actor_01/03-01-...  \n","1     /content/drive/MyDrive/ravdess/Actor_01/03-01-...  \n","2     /content/drive/MyDrive/ravdess/Actor_01/03-01-...  \n","3     /content/drive/MyDrive/ravdess/Actor_01/03-01-...  \n","4     /content/drive/MyDrive/ravdess/Actor_01/03-01-...  \n","...                                                 ...  \n","1435  /content/drive/MyDrive/ravdess/Actor_24/03-01-...  \n","1436  /content/drive/MyDrive/ravdess/Actor_24/03-01-...  \n","1437  /content/drive/MyDrive/ravdess/Actor_24/03-01-...  \n","1438  /content/drive/MyDrive/ravdess/Actor_24/03-01-...  \n","1439  /content/drive/MyDrive/ravdess/Actor_24/03-01-...  \n","\n","[1440 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-96d16748-484f-4273-b8bf-f8166ebb2c02\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","      <th>source</th>\n","      <th>path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>male_neutral</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_01/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>male_neutral</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_01/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>male_sad</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_01/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>male_happy</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_01/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>male_angry</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_01/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1435</th>\n","      <td>female_angry</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_24/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>1436</th>\n","      <td>female_surprise</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_24/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>1437</th>\n","      <td>female_surprise</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_24/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>1438</th>\n","      <td>female_surprise</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_24/03-01-...</td>\n","    </tr>\n","    <tr>\n","      <th>1439</th>\n","      <td>female_surprise</td>\n","      <td>RAVDESS</td>\n","      <td>/content/drive/MyDrive/ravdess/Actor_24/03-01-...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1440 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96d16748-484f-4273-b8bf-f8166ebb2c02')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-96d16748-484f-4273-b8bf-f8166ebb2c02 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-96d16748-484f-4273-b8bf-f8166ebb2c02');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["RAV_df"],"id":"103b71d5"},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3223,"status":"ok","timestamp":1688586491243,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"193ac8f8","outputId":"893bbce1-1911-4d15-e287-53425ff52dfe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["female_fear        400\n","female_surprise    400\n","female_sad         400\n","female_angry       400\n","female_disgust     400\n","female_happy       400\n","female_neutral     400\n","Name: labels, dtype: int64"]},"metadata":{},"execution_count":12}],"source":["dir_list = os.listdir(TESS)\n","dir_list.sort()\n","\n","path = []\n","emotion = []\n","\n","for directory in dir_list:\n","    if directory == \"tess toronto emotional speech set data\":\n","        continue\n","    fname = os.listdir(TESS + directory)\n","\n","    for f in fname:\n","        if directory == 'OAF_angry' or directory == 'YAF_angry':\n","            emotion.append('female_angry')\n","        elif directory == 'OAF_disgust' or directory == 'YAF_disgust':\n","            emotion.append('female_disgust')\n","        elif directory == 'OAF_Fear' or directory == 'YAF_fear':\n","            emotion.append('female_fear')\n","        elif directory == 'OAF_happy' or directory == 'YAF_happy':\n","            emotion.append('female_happy')\n","        elif directory == 'OAF_neutral' or directory == 'YAF_neutral':\n","            emotion.append('female_neutral')\n","        elif directory == 'OAF_Pleasant_surprise' or directory == 'YAF_pleasant_surprised':\n","            emotion.append('female_surprise')\n","        elif directory == 'OAF_Sad' or directory == 'YAF_sad':\n","            emotion.append('female_sad')\n","        else:\n","            emotion.append('Unknown')\n","        path.append(TESS + directory + \"/\" + f)\n","\n","TESS_df = pd.DataFrame(emotion, columns = ['labels'])\n","TESS_df['source'] = 'TESS'\n","TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])], axis=1)\n","TESS_df.labels.value_counts()"],"id":"193ac8f8"},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1688586491784,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"15a2ae2a","outputId":"71e81eae-d297-4c50-f112-dc73c90c39ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["           labels source                                               path\n","0     female_fear   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","1     female_fear   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","2     female_fear   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","3     female_fear   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","4     female_fear   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","...           ...    ...                                                ...\n","2795   female_sad   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","2796   female_sad   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","2797   female_sad   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","2798   female_sad   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","2799   female_sad   TESS  /content/drive/MyDrive/tess/TESS Toronto emoti...\n","\n","[2800 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-029bec33-40d1-40d4-be9a-44817a271866\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","      <th>source</th>\n","      <th>path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>female_fear</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>female_fear</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>female_fear</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>female_fear</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>female_fear</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2795</th>\n","      <td>female_sad</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>2796</th>\n","      <td>female_sad</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>2797</th>\n","      <td>female_sad</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>2798</th>\n","      <td>female_sad</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","    <tr>\n","      <th>2799</th>\n","      <td>female_sad</td>\n","      <td>TESS</td>\n","      <td>/content/drive/MyDrive/tess/TESS Toronto emoti...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2800 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-029bec33-40d1-40d4-be9a-44817a271866')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-029bec33-40d1-40d4-be9a-44817a271866 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-029bec33-40d1-40d4-be9a-44817a271866');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}],"source":["TESS_df"],"id":"15a2ae2a"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32808,"status":"ok","timestamp":1688586524585,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"bae60eca","outputId":"0a9d0ece-00d1-4495-dca2-cabb256831fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["male_angry        671\n","male_disgust      671\n","male_fear         671\n","male_happy        671\n","male_sad          671\n","female_angry      600\n","female_disgust    600\n","female_fear       600\n","female_happy      600\n","female_sad        600\n","male_neutral      575\n","female_neutral    512\n","Name: labels, dtype: int64"]},"metadata":{},"execution_count":14}],"source":["dir_list = os.listdir(CREMA)\n","dir_list.sort()\n","\n","gender = []\n","emotion = []\n","path = []\n","female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n","          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n","\n","for i in dir_list:\n","    part = i.split('_')\n","    if int(part[0]) in female:\n","        temp = 'female'\n","    else:\n","        temp = 'male'\n","    gender.append(temp)\n","    if part[2] == 'SAD' and temp == 'male':\n","        emotion.append('male_sad')\n","    elif part[2] == 'ANG' and temp == 'male':\n","        emotion.append('male_angry')\n","    elif part[2] == 'DIS' and temp == 'male':\n","        emotion.append('male_disgust')\n","    elif part[2] == 'FEA' and temp == 'male':\n","        emotion.append('male_fear')\n","    elif part[2] == 'HAP' and temp == 'male':\n","        emotion.append('male_happy')\n","    elif part[2] == 'NEU' and temp == 'male':\n","        emotion.append('male_neutral')\n","    elif part[2] == 'SAD' and temp == 'female':\n","        emotion.append('female_sad')\n","    elif part[2] == 'ANG' and temp == 'female':\n","        emotion.append('female_angry')\n","    elif part[2] == 'DIS' and temp == 'female':\n","        emotion.append('female_disgust')\n","    elif part[2] == 'FEA' and temp == 'female':\n","        emotion.append('female_fear')\n","    elif part[2] == 'HAP' and temp == 'female':\n","        emotion.append('female_happy')\n","    elif part[2] == 'NEU' and temp == 'female':\n","        emotion.append('female_neutral')\n","    else:\n","        emotion.append('Unknown')\n","    path.append(CREMA + i)\n","\n","CREMA_df = pd.DataFrame(emotion, columns = ['labels'])\n","CREMA_df['source'] = 'CREMA'\n","CREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n","CREMA_df.labels.value_counts()"],"id":"bae60eca"},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1688586524587,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"41620e02","outputId":"2482534b-ad33-4405-e713-ef53f006d01a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              labels source                                               path\n","0         male_angry  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1001_DF...\n","1       male_disgust  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1001_DF...\n","2          male_fear  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1001_DF...\n","3         male_happy  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1001_DF...\n","4       male_neutral  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1001_DF...\n","...              ...    ...                                                ...\n","7437  female_disgust  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...\n","7438     female_fear  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...\n","7439    female_happy  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...\n","7440  female_neutral  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...\n","7441      female_sad  CREMA  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...\n","\n","[7442 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-cd90b444-d5d4-474a-877b-c92f7ab9a5fc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","      <th>source</th>\n","      <th>path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>male_angry</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1001_DF...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>male_disgust</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1001_DF...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>male_fear</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1001_DF...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>male_happy</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1001_DF...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>male_neutral</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1001_DF...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7437</th>\n","      <td>female_disgust</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","    <tr>\n","      <th>7438</th>\n","      <td>female_fear</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","    <tr>\n","      <th>7439</th>\n","      <td>female_happy</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","    <tr>\n","      <th>7440</th>\n","      <td>female_neutral</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","    <tr>\n","      <th>7441</th>\n","      <td>female_sad</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7442 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd90b444-d5d4-474a-877b-c92f7ab9a5fc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cd90b444-d5d4-474a-877b-c92f7ab9a5fc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cd90b444-d5d4-474a-877b-c92f7ab9a5fc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}],"source":["CREMA_df"],"id":"41620e02"},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1688586524590,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"f45d887a","outputId":"69e0f9cf-a517-4ace-cea8-c9b776ed36ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["female_happy       1096\n","female_sad         1096\n","female_angry       1096\n","female_disgust     1096\n","female_fear        1096\n","female_neutral     1056\n","male_neutral        839\n","male_angry          827\n","male_happy          827\n","male_disgust        827\n","male_fear           827\n","male_sad            827\n","female_surprise     496\n","male_surprise       156\n","Name: labels, dtype: int64\n"]}],"source":["df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\n","df = df.reset_index(drop=True)\n","print(df.labels.value_counts())"],"id":"f45d887a"},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":94,"status":"ok","timestamp":1688586524591,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"ee5205c3","outputId":"d39fcc25-e47a-40ac-deb5-ba8b34f34cd5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               labels source  \\\n","0          male_angry  SAVEE   \n","1          male_angry  SAVEE   \n","2          male_angry  SAVEE   \n","3          male_angry  SAVEE   \n","4          male_angry  SAVEE   \n","...               ...    ...   \n","12157  female_disgust  CREMA   \n","12158     female_fear  CREMA   \n","12159    female_happy  CREMA   \n","12160  female_neutral  CREMA   \n","12161      female_sad  CREMA   \n","\n","                                                    path  \n","0            /content/drive/MyDrive/savee/ALL/DC_a01.wav  \n","1            /content/drive/MyDrive/savee/ALL/DC_a08.wav  \n","2            /content/drive/MyDrive/savee/ALL/DC_a09.wav  \n","3            /content/drive/MyDrive/savee/ALL/DC_a10.wav  \n","4            /content/drive/MyDrive/savee/ALL/DC_a02.wav  \n","...                                                  ...  \n","12157  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...  \n","12158  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...  \n","12159  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...  \n","12160  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...  \n","12161  /content/drive/MyDrive/cremad/AudioWAV/1091_WS...  \n","\n","[12162 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-fe2adf03-5607-4b14-b2e2-f86d49aa22ea\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","      <th>source</th>\n","      <th>path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a01.wav</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a08.wav</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a09.wav</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a10.wav</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>male_angry</td>\n","      <td>SAVEE</td>\n","      <td>/content/drive/MyDrive/savee/ALL/DC_a02.wav</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12157</th>\n","      <td>female_disgust</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","    <tr>\n","      <th>12158</th>\n","      <td>female_fear</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","    <tr>\n","      <th>12159</th>\n","      <td>female_happy</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","    <tr>\n","      <th>12160</th>\n","      <td>female_neutral</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","    <tr>\n","      <th>12161</th>\n","      <td>female_sad</td>\n","      <td>CREMA</td>\n","      <td>/content/drive/MyDrive/cremad/AudioWAV/1091_WS...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12162 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe2adf03-5607-4b14-b2e2-f86d49aa22ea')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fe2adf03-5607-4b14-b2e2-f86d49aa22ea button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fe2adf03-5607-4b14-b2e2-f86d49aa22ea');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}],"source":["df"],"id":"ee5205c3"},{"cell_type":"markdown","metadata":{"id":"7b4565d3"},"source":["## 1.2 Convert Label to classes"],"id":"7b4565d3"},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":90,"status":"ok","timestamp":1688586524592,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"4f34344e"},"outputs":[],"source":["labels = ['female_angry', 'female_disgust', 'female_fear', 'female_happy',\n"," 'female_neutral', 'female_sad', 'female_surprise', 'male_angry',\n"," 'male_disgust', 'male_fear', 'male_happy', 'male_neutral', 'male_sad',\n"," 'male_surprise']"],"id":"4f34344e"},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":90,"status":"ok","timestamp":1688586524593,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"0c00c5f3"},"outputs":[],"source":["def label_to_class(label, labels_list):\n","    if label in labels_list:\n","        return labels_list.index(label)"],"id":"0c00c5f3"},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":89,"status":"ok","timestamp":1688586524593,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"fba6683c"},"outputs":[],"source":["df['label_class'] = df['labels'].apply(lambda x: label_to_class(x, labels))"],"id":"fba6683c"},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83,"status":"ok","timestamp":1688586524594,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"48484d1f","outputId":"361eb8b1-fe61-486e-fe99-83eb44d2c671"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3     1096\n","5     1096\n","0     1096\n","1     1096\n","2     1096\n","4     1056\n","11     839\n","7      827\n","10     827\n","8      827\n","9      827\n","12     827\n","6      496\n","13     156\n","Name: label_class, dtype: int64"]},"metadata":{},"execution_count":21}],"source":["df.label_class.value_counts()"],"id":"48484d1f"},{"cell_type":"markdown","metadata":{"id":"7ed2e20c"},"source":["## 1.3 Calculate duration of each clips"],"id":"7ed2e20c"},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":46,"status":"ok","timestamp":1688586524594,"user":{"displayName":"Majed Almarzouqi","userId":"17169123754813928493"},"user_tz":-240},"id":"3cd38ef3"},"outputs":[],"source":["from pydub import AudioSegment\n","\n","def calculate_duration(audio_file_path):\n","    audio = AudioSegment.from_file(audio_file_path)\n","\n","    # Calculate duration in seconds\n","    duration = audio.duration_seconds\n","    return duration"],"id":"3cd38ef3"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7fa470a2"},"outputs":[],"source":["df['duration'] = df['path'].apply(lambda x: calculate_duration(x))"],"id":"7fa470a2"},{"cell_type":"markdown","metadata":{"id":"25808571"},"source":["## 2. Data Visualization"],"id":"25808571"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7c817287"},"outputs":[],"source":["# import pandas as pd\n","# df = pd.read_csv(\"/content/drive/MyDrive/Kaggle_emotional_data_path.csv\", index_col=0)\n","df"],"id":"7c817287"},{"cell_type":"markdown","metadata":{"id":"23d9975d"},"source":["## 2.1 Shuffle the datasets"],"id":"23d9975d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"f01e11e7"},"outputs":[],"source":["from sklearn.utils import shuffle\n","\n","df = shuffle(df)\n","df"],"id":"f01e11e7"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"50f2f3d4"},"outputs":[],"source":["df[df['duration']>4]"],"id":"50f2f3d4"},{"cell_type":"markdown","metadata":{"id":"17582bb6"},"source":["## 2.2 Drop all long clips (audio > 4 sec)\n","\n","Since we got over 10k data, we can cut some long clips off to save some computational power"],"id":"17582bb6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5639262b"},"outputs":[],"source":["df = df.drop(df[df.duration > 4].index)"],"id":"5639262b"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2f6ce2b0"},"outputs":[],"source":["df.sort_values(by=\"duration\")"],"id":"2f6ce2b0"},{"cell_type":"markdown","metadata":{"id":"df7cc9dd"},"source":["## 2.3 Create functions for visualize data"],"id":"df7cc9dd"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"a13f67b8"},"outputs":[],"source":["def load_audio(AUDIO_PATH):\n","    audio, sr = librosa.load(AUDIO_PATH, sr=44100)\n","    return audio, sr\n","\n","def wav2melSpec(AUDIO_PATH):\n","    audio, sr = librosa.load(AUDIO_PATH)\n","    return librosa.feature.melspectrogram(y=audio, sr=sr)\n","\n","def imgSpec(ms_feature):\n","    fig, ax = plt.subplots()\n","    ms_dB = librosa.power_to_db(ms_feature, ref=np.max)\n","    print(ms_feature.shape)\n","    img = librosa.display.specshow(ms_dB, x_axis='time', y_axis='mel', ax=ax)\n","    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n","    ax.set(title='Mel-frequency spectrogram');\n","\n","def hear_audio(AUDIO_PATH):\n","    audio, sr = librosa.load(AUDIO_PATH)\n","\n","    print(\"\\t\", end=\"\")\n","    ipd.display(ipd.Audio(data=audio, rate=sr))\n","\n","\n","def get_audio_info(path, show_melspec=False, label=None):\n","    spec = wav2melSpec(path)\n","    if label is not None:\n","        print(\"Label:\", label)\n","    if show_melspec is not False:\n","        imgSpec(spec)\n","    hear_audio(path)"],"id":"a13f67b8"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"f3ac3290"},"outputs":[],"source":["get_audio_info('/content/drive/MyDrive/savee/ALL/JK_f11.wav', show_melspec=True)"],"id":"f3ac3290"},{"cell_type":"markdown","metadata":{"id":"068e260a"},"source":["# 3. Create DataLoader"],"id":"068e260a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8d8ae66a"},"outputs":[],"source":["from torchaudio import transforms\n","\n","class AudioDataset(Dataset):\n","    def __init__(self, df, data_col, label_col, max_length=4*16000, new_sr=16000, train=True, train_size=0.80):\n","        self.file_path_list = df[data_col].tolist()\n","        self.label_list = df[label_col].tolist()\n","        self.max_length = max_length\n","        self.new_sr = new_sr\n","\n","        total_len = len(self.file_path_list)\n","\n","        if train:\n","            self.file_path_list, self.label_list = self.file_path_list[:int(train_size * total_len)], self.label_list[:int(train_size * total_len)]\n","        else:\n","            self.file_path_list, self.label_list = self.file_path_list[int(train_size * total_len):], self.label_list[int(train_size * total_len):]\n","\n","    def __len__(self):\n","        return len(self.file_path_list)\n","\n","    def __getitem__(self, idx):\n","        audio, sample_rate = librosa.load(self.file_path_list[idx])\n","        if sample_rate != self.new_sr:\n","            audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=self.new_sr)\n","        label = self.label_list[idx]\n","\n","        # Pad or trim the audio signal to the desired length\n","        # Pad the audio tensor with zeros to a fixed length of 160000\n","        desired_length = self.max_length\n","        if len(audio) < desired_length:\n","            padding = desired_length - len(audio)\n","            audio = np.pad(audio, (0, padding), 'constant')\n","\n","        return audio, label, self.file_path_list[idx]\n"],"id":"8d8ae66a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"895e84c2"},"outputs":[],"source":["BATCH_SIZE = 16\n","\n","train_dataset = AudioDataset(df, 'path', 'label_class', train=True)\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","test_dataset = AudioDataset(df, 'path', 'label_class', train=False)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","print(len(train_dataset), len(test_dataset))"],"id":"895e84c2"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"b5fc5b8a"},"outputs":[],"source":["# # Visualize the shape of the data in dataloader\n","# max_length = 0\n","# for batch_idx, (audio_batch, label_batch, file_path) in enumerate(train_dataset):\n","#     print(audio_batch.shape, label_batch, file_path)\n","#     if audio_batch.shape[0] > max_length:\n","#         max_length = audio_batch.shape[0]\n","# print(f\"max length = {max_length}\")"],"id":"b5fc5b8a"},{"cell_type":"markdown","metadata":{"id":"6db28597"},"source":["# 4. Model Building"],"id":"6db28597"},{"cell_type":"markdown","metadata":{"id":"bc935315"},"source":["## 4.1 HuBERT from Transformers (huggingface)\n","Assume that the audio is represented as an **numpy array** with a shape of (time_frames, )\n","\n","- output_size of Wav2Vec2FeatureExtractor = (batch_size, num_channels, time_frames)\n","  - Remove num_channels before go into the hubert model\n","\n","- output_size of hubert = (batch_size, time_frames, frequency_bins).\n","\n","\n","\n","***Remarks:*** The number of channels is 1 because audio typically only has one channel."],"id":"bc935315"},{"cell_type":"markdown","metadata":{"id":"7340a5b0"},"source":["#### Model Architecture"],"id":"7340a5b0"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cc5ab09a"},"outputs":[],"source":["import torch\n","import torchaudio\n","from transformers import HubertModel, Wav2Vec2FeatureExtractor\n","\n","# Create feature extractor\n","model_id = \"facebook/hubert-base-ls960\"\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)\n","\n","# Load the Hubert model and tokenizer\n","hubert_base = HubertModel.from_pretrained(model_id)\n","\n","\n","class HubertAudioModel(torch.nn.Module):\n","    def __init__(self, hubert_model=hubert_base):\n","        super().__init__()\n","        self.hubert = hubert_model\n","        self.fc1 = torch.nn.Linear(199*768, 256)\n","        self.fc2 = torch.nn.Linear(256, 14)\n","\n","    def forward(self, audio_array):\n","        # Resample the audio to the required sample rate (16kHz for Hubert)\n","        # audio_array = librosa.load(audio_file, sr=16000, mono=False)[0]\n","        # print(f\"audio_array shape before Wav2Vec: {audio_array.shape}\")\n","        input = feature_extractor(audio_array,\n","                           sampling_rate=16000,\n","                           padding=True,\n","                           return_tensors='pt').to(device)\n","\n","        # print(f\"input.input_values shape after Wav2Vec: {input.input_values.shape}\")\n","\n","        input = input.input_values.squeeze(dim=0)\n","        # print(f\"input shape after squeeze: {input.shape}\")\n","\n","        # Pass the spectrogram through the Hubert model\n","        output = self.hubert(input)\n","        # print(f\"output.last_hidden_state shape after hubert: {output.last_hidden_state.shape}\")\n","\n","        # Flatten the output of the Hubert model\n","        output = torch.flatten(output.last_hidden_state, start_dim=1)\n","\n","        # print(f\"output shape after flatten: {output.shape}\")\n","\n","        # Pass the flattened output through two dense layers\n","        output = torch.nn.functional.relu(self.fc1(output))\n","        output = self.fc2(output)\n","\n","        return output\n"],"id":"cc5ab09a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"523c89d9"},"outputs":[],"source":["model = HubertAudioModel().to(device)\n","\n","next(model.parameters()).device"],"id":"523c89d9"},{"cell_type":"markdown","metadata":{"id":"4ee6c672"},"source":["# 5. Setup Loss function, Optimizers & Metrics: Accuracy function"],"id":"4ee6c672"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1a9a3405"},"outputs":[],"source":["# Loss function\n","loss_fn = nn.CrossEntropyLoss() # Multi-category loss\n","\n","# Create an optimizer\n","optimizer = torch.optim.SGD(params=model.parameters(), lr=0.005)"],"id":"1a9a3405"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8642ba45"},"outputs":[],"source":["# Calculate accuracy (a classification metric)\n","def accuracy_fn(y_true, y_pred):\n","    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n","    acc = (correct / len(y_pred)) * 100\n","    return acc"],"id":"8642ba45"},{"cell_type":"markdown","metadata":{"id":"365b0289"},"source":["# 6. Create Training & Testing Loop"],"id":"365b0289"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"a5645c4f"},"outputs":[],"source":["def train_step(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, accuracy_fn,\n","               device: torch.device = device):\n","\n","    train_loss, train_acc = 0, 0\n","\n","    # Put model into training mode\n","    model.train()\n","\n","    # Add a loop to loop through training batches\n","    for batch, (X, y, file_path) in enumerate(data_loader):\n","        # Put data on target device\n","        X, y = X.to(device), y.to(device)\n","\n","        # 1. Forward pass\n","        y_logits = model(X).to(device)\n","        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1).to(device)\n","\n","        # 2.1 Calculate loss (per batch)\n","        loss = loss_fn(y_logits, y)\n","        train_loss += loss # accumulatively add up the loss per epoch\n","\n","        # 2.2 Calculate accuracy (per batch)\n","        acc = accuracy_fn(y_true=y, y_pred=y_pred)\n","        train_acc += acc\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","        if batch % 50 == 0:\n","            sample = random.randint(0, BATCH_SIZE-2)\n","            print(f\"\\tBatch {batch}: Train loss: {loss:.5f} | Train accuracy : {acc:.2f}%\")\n","            get_audio_info(file_path[sample], label=y_pred[sample].item())\n","            print(\"----------------------------------------\")\n","\n","    # Divide total train loss and accuracy by length of train dataloader (average loss per batch per epoch)\n","    train_loss /= len(data_loader)\n","    train_acc /= len(data_loader)\n","\n","\n","    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\\n\")"],"id":"a5645c4f"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"d66b1941"},"outputs":[],"source":["def test_step(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module, accuracy_fn, device: torch.device = device):\n","    ### Testing\n","    # Setup variables for accumulatively adding up loss and accuracy\n","    test_loss, test_acc = 0, 0\n","\n","    # Put the model in eval mode\n","    model.eval()\n","\n","    # Turn on inference mode context manager\n","    with torch.inference_mode():\n","        for batch, (X, y, file_path) in enumerate(data_loader):\n","            # Send the data to the target device\n","            X, y = X.to(device), y.to(device)\n","\n","            # 1. Forward pass\n","            test_logits = model(X)\n","            test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1).to(device)\n","\n","            # 2. Calculate loss / acc (accumatively)\n","            loss = loss_fn(test_logits, y)\n","            test_loss += loss\n","\n","            # 3. Calculate accuracy (preds need to be same as y_true)\n","            acc = accuracy_fn(y_true=y, y_pred=test_pred)\n","            test_acc += acc\n","\n","            if batch % 50 == 0:\n","                sample = random.randint(0, BATCH_SIZE-2)\n","                print(f\"\\tBatch {batch}: Test loss: {test_loss:.5f} | Test accuracy : {acc:.2f}%\")\n","                get_audio_info(file_path[sample], label=test_pred[sample].item())\n","                print(\"----------------------------------------\")\n","\n","\n","        # Calculations on test metrics need to happen inside torch.inference_mode()\n","        # Divide total test loss / accuracy by length of test dataloader (per batch)\n","        test_loss /= len(data_loader)\n","        test_acc /= len(data_loader)\n","\n","        print(f\"Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n"],"id":"d66b1941"},{"cell_type":"markdown","metadata":{"id":"d0a6f52e"},"source":["# 7. Train the model"],"id":"d0a6f52e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"92f20c7e"},"outputs":[],"source":["from tqdm.auto import tqdm\n","\n","# Set the seed\n","# torch.manual_seed(42)\n","\n","# Set epochs\n","epochs = 3\n","\n","# Create a optimization and evaluation loop using train_step() and test_step()\n","for epoch in tqdm(range(epochs)):\n","    print(f\"Epoch: {epoch}\\n-------\")\n","    train_step(model=model, data_loader=train_dataloader, loss_fn=loss_fn,\n","               optimizer=optimizer, accuracy_fn=accuracy_fn, device=device)\n","\n","    test_step(model=model, data_loader=test_dataloader, loss_fn=loss_fn,\n","              accuracy_fn=accuracy_fn, device=device)\n"],"id":"92f20c7e"},{"cell_type":"markdown","metadata":{"id":"e22c2146"},"source":["# 8. Evaluate the model"],"id":"e22c2146"},{"cell_type":"code","execution_count":null,"metadata":{"id":"48a69c2f"},"outputs":[],"source":["torch.manual_seed(42)\n","def eval_model(model: torch.nn.Module,\n","               data_loader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               accuracy_fn, device=device):\n","    \"\"\"Return a dictionary containing the results of model predicting on data_loader\"\"\"\n","    loss, acc = 0, 0\n","    model.eval()\n","    with torch.inference_mode():\n","        for X, y, file_path in data_loader:\n","            # Put data on target device\n","            X, y = X.to(device), y.to(device)\n","            # Make predictions\n","            y_logits = model(X)\n","            y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1).to(device)\n","\n","            # Accumulate the loss and acc values per batch\n","            loss += loss_fn(y_logits, y)\n","            acc += accuracy_fn(y_pred=y_pred, y_true=y)\n","\n","        # Scale loss and acc to find the average loss/acc per batch\n","        loss /= len(data_loader)\n","        acc /= len(data_loader)\n","\n","    return {\"model_name\": model.__class__.__name__,\n","            \"model_loss\": loss.item(),\n","            \"model_acc\": acc}"],"id":"48a69c2f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7d706223"},"outputs":[],"source":["model_result = eval_model(model, test_dataloader, loss_fn, accuracy_fn)"],"id":"7d706223"},{"cell_type":"code","execution_count":null,"metadata":{"id":"78874a0f"},"outputs":[],"source":["model_result"],"id":"78874a0f"},{"cell_type":"markdown","metadata":{"id":"4ebafd1c"},"source":["# 9. Make and Evaluate random predictions"],"id":"4ebafd1c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8177db1"},"outputs":[],"source":["def resample(sample, sample_rate, new_sample_rate):\n","      return librosa.resample(sample, orig_sr=sample_rate, target_sr=16000)\n","\n","def pad(sample, desired_length=4*16000):\n","    # Pad the audio tensor with zeros to a fixed length of 16000*8s\n","    if len(sample) < desired_length:\n","        padding = desired_length - len(sample)\n","        sample = np.pad(sample, (0, padding), 'constant')\n","    elif len(sample) > desired_length:\n","        sample = sample[:desired_length]\n","    return sample"],"id":"e8177db1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3584ef79"},"outputs":[],"source":["def make_predictions(model, data, device=device):\n","    pred_probs = []\n","    model.eval()\n","    with torch.inference_mode():\n","        for sample in data:\n","            # Prepare the sample (add a batch dimension and pass to target device)\n","            sample = torch.unsqueeze(sample, dim=0).to(device)\n","\n","            # Forward pass (model outputs raw logits)\n","            pred_logits = model(sample)\n","\n","            # Get prediction probability (logit -> prediction probability)\n","            pred_prob = torch.softmax(pred_logits.squeeze(), dim=0)\n","\n","            # Get pred_probs off the GPU for further calculations\n","            pred_probs.append(pred_prob.cpu())\n","\n","    # Stack the pred_probs to turn list into a tensor\n","    return torch.stack(pred_probs)"],"id":"3584ef79"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8a5efb54"},"outputs":[],"source":["import random\n","random.seed(42)\n","\n","test_data = list(zip(df.path, df.label_class))\n","test_samples = []\n","test_labels = []\n","\n","# Pick k=9 samples randomly\n","for sample_path, label in random.sample(test_data, k=9):\n","    sample, sample_rate = load_audio(sample_path)\n","    if sample_rate != 16000:\n","        sample = resample(sample, sample_rate, 16000)\n","    sample = pad(sample)\n","    sample_tensor = torch.from_numpy(sample)\n","    test_samples.append(sample_tensor)\n","    test_labels.append(label)\n","\n","# View the first sample shape\n","test_samples[0].shape"],"id":"8a5efb54"},{"cell_type":"code","execution_count":null,"metadata":{"id":"63f72f77"},"outputs":[],"source":["pred_probs = make_predictions(model=model,\n","                              data=test_samples)\n","pred_probs"],"id":"63f72f77"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3d55bd8"},"outputs":[],"source":["test_labels"],"id":"d3d55bd8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0a885f0"},"outputs":[],"source":["pred_classes = pred_probs.argmax(dim=1)\n","pred_classes"],"id":"a0a885f0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"810fcb6f"},"outputs":[],"source":["# Plot predictions\n","plt.figure(figsize=(9,9))\n","nrows = 3\n","ncols = 3\n","\n","class_names = ['female_angry', 'female_disgust', 'female_fear', 'female_happy',\n"," 'female_neutral', 'female_sad', 'female_surprise', 'male_angry',\n"," 'male_disgust', 'male_fear', 'male_happy', 'male_neutral', 'male_sad',\n"," 'male_surprise']\n","\n","for i, sample in enumerate(test_samples):\n","    # Create subplot\n","    plt.subplot(nrows, ncols, i+1)\n","\n","    # Plot the audio\n","    plt.plot(sample)\n","\n","    # Find the prediction (in text form)\n","    pred_label = class_names[pred_classes[i]]\n","\n","    # Get the true label (in text form)\n","    true_label = class_names[test_labels[i]]\n","\n","    # Create the title of the plot\n","    title = f\"Pred: {pred_label} | Truth: {true_label}\"\n","\n","    # # Display the target audio\n","    # ipd.display(ipd.Audio(data=sample, rate=16000))\n","\n","    # Check for equality between pred and true labels and change color of title text\n","    if pred_label == true_label:\n","        plt.title(title, fontsize=10, c=\"g\")\n","    else:\n","        plt.title(title, fontsize=10, c=\"r\")\n","\n","    # Adjust subplot\n","    plt.subplots_adjust(left=0.1,\n","                        bottom=0.1,\n","                        right=1.0,\n","                        top=1.0,\n","                        wspace=0.5,\n","                        hspace=0.5)\n","\n","    plt.axis(False)"],"id":"810fcb6f"},{"cell_type":"markdown","metadata":{"id":"6299d5b5"},"source":["## 9. Confusion Matrix"],"id":"6299d5b5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e56b44d2"},"outputs":[],"source":["!pip install -q torchmetrics -U mlxtend"],"id":"e56b44d2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dadbb1dc"},"outputs":[],"source":["def make_predictions(model, data_loader, device=device):\n","    y_preds = []\n","    y_labels =[]\n","    model.eval()\n","    with torch.inference_mode():\n","        for X, y, file_path in tqdm(data_loader, desc=\"Making predictions...\"):\n","            # Prepare the sample (add a batch dimension and pass to target device)\n","            X, y = X.to(device), y.to(device)\n","\n","            # Forward pass (model outputs raw logits)\n","            y_logits = model(X)\n","\n","            # Get prediction from logit -> prediction probability -> prediction labels\n","            # dimension = 1: each row sum up => 1\n","            y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1).to(device)\n","\n","            # Get pred_probs off the GPU for further calculations\n","            y_preds.append(y_pred.cpu())\n","            y_labels.append(y.cpu())\n","\n","    # Stack the pred_probs to turn list into a tensor\n","    y_pred_tensor = torch.cat(y_preds)\n","    y_label_tensor = torch.cat(y_labels)\n","    return y_pred_tensor, y_label_tensor"],"id":"dadbb1dc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f881aedf"},"outputs":[],"source":["y_pred_tensor, y_label_tensor = make_predictions(model, test_dataloader)\n","y_pred_tensor[:10]"],"id":"f881aedf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d7108bf"},"outputs":[],"source":["y_label_tensor[:10]"],"id":"4d7108bf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a198857b"},"outputs":[],"source":["from torchmetrics import ConfusionMatrix\n","from mlxtend.plotting import plot_confusion_matrix\n","import numpy as np\n","\n","class_names = ['female_angry', 'female_disgust', 'female_fear', 'female_happy',\n","               'female_neutral', 'female_sad', 'female_surprise', 'male_angry',\n","               'male_disgust', 'male_fear', 'male_happy', 'male_neutral',\n","               'male_sad', 'male_surprise']\n","\n","# default threshold = 0.5\n","conf_mat = ConfusionMatrix(task='multiclass', num_classes=14)\n","conf_mat_tensor = conf_mat(preds=y_pred_tensor, target=y_label_tensor)\n","\n","# Calculate row-wise sums\n","row_sums = np.sum(conf_mat_tensor.numpy(), axis=1)\n","\n","# Normalize the confusion matrix\n","conf_mat_tensor = conf_mat_tensor.numpy() / row_sums[:, np.newaxis]\n","\n","fig, ax = plot_confusion_matrix(\n","    conf_mat=conf_mat_tensor,\n","    class_names=class_names,\n","    figsize=(10, 7),\n","    show_absolute=False,  # Show relative values instead of absolute\n","    show_normed=True  # Show values as percentages\n",")\n"],"id":"a198857b"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":2104.869921,"end_time":"2023-06-18T10:29:15.938083","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-06-18T09:54:11.068162","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}